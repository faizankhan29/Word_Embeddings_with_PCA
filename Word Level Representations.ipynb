{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of the Repository: (Work in Progress)\n",
    "\n",
    "The idea of the blog/repository is to:\n",
    "\n",
    "1) Explore PCA for text visualizations -- words, sentences, documents.\n",
    "\n",
    "2) Understand how using PCA helps improve text classification.\n",
    "\n",
    "3) Explore the Spotify Annoy library and test the text classification improvement using the same.\n",
    "\n",
    "4) (Optional) Explore the TSNE library for visualization and insights -- words, sentences, documents, text classification outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "file=open('data/test.ft.txt','r',encoding=\"utf-8\")\n",
    "\n",
    "g=file.read()\n",
    "\n",
    "sentences=sent_tokenize(g)\n",
    "\n",
    "wn=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words=stopwords.words('english')\n",
    "    stop_words.extend(['__label__1','__label__2'])\n",
    "    tokens=[token.lower() for token in tokens if token not in stop_words]\n",
    "    tokens=[re.sub(r'[^A-Za-z]+','',token) for token in tokens]\n",
    "    tokens=[wn.lemmatize(token) for token in tokens]\n",
    "    return tokens  \n",
    "\n",
    "\n",
    "text_tokens=[]\n",
    "for item in sentences[0:1000]:\n",
    "    tokens = preprocess_text(item)\n",
    "    temp = \" \".join(tokens)\n",
    "    text_tokens.append(temp)    \n",
    "\n",
    "\n",
    "\n",
    "word_dist = FreqDist()\n",
    "for s in text_tokens:\n",
    "    word_dist.update(s.split())\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "text=''\n",
    "for sent in text_tokens:\n",
    "    text=text+sent\n",
    "\n",
    "tokens=word_tokenize(text)    \n",
    "bigrams = ngrams(tokens,2)\n",
    "\n",
    "bigram_dict=dict(Counter(bigrams))\n",
    "\n",
    "final_bigram_dict={}\n",
    "for key,value in bigram_dict.items():\n",
    "    new_key=\" \".join(key)\n",
    "    final_bigram_dict[new_key]=value\n",
    "\n",
    "unigram_index= CountVectorizer(ngram_range=(1,1))\n",
    "unigram_index.fit_transform(text_tokens)\n",
    "unigram_dist = unigram_index.vocabulary_\n",
    "\n",
    "\n",
    "def pmi(word1, word2 ,unigram_freq, bigram_freq):\n",
    "    #print(word1,word2)\n",
    "    prob_word1 = unigram_freq[word1]/float(sum(unigram_freq.values()))\n",
    "    #print(prob_word1)\n",
    "    prob_word2 = unigram_freq[word2]/float(sum(unigram_freq.values()))\n",
    "    #print(prob_word2)\n",
    "    prob_word1_word2 = bigram_freq[\" \".join([word1,word2])]/float(sum(bigram_freq.values()))\n",
    "    #print(prob_word1_word2)\n",
    "    ratio = prob_word1_word2/float(prob_word1*prob_word2)\n",
    "    #print(word1,word2,prob_word1,prob_word2)\n",
    "    if ratio==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log(ratio,2)\n",
    "\n",
    "pmi_dict={}\n",
    "for key in final_bigram_dict.keys():\n",
    "    first_word = key.split()[0]\n",
    "    second_word = key.split()[1]\n",
    "    if (first_word in word_dist.keys()) and (second_word in word_dist.keys()):\n",
    "        pmi_dict[key]=pmi(key.split()[0],key.split()[1],word_dist,final_bigram_dict)\n",
    "    else:\n",
    "        pmi_dict[key]=0\n",
    "\n",
    "\n",
    "start = '\\s'\n",
    "end= '\\e'\n",
    "\n",
    "    \n",
    "context_word_pairs={}\n",
    "for story_id in range(0,len(text_tokens)):\n",
    "    text_tokens[story_id] = start +' '+ text_tokens[story_id] +' '+ end\n",
    "    list_of_words=text_tokens[story_id].split()\n",
    "    context_word_pairs[story_id] = {}\n",
    "    for word_index in range(1,len(list_of_words)-1):\n",
    "        context_word_pairs[story_id][list_of_words[word_index]]=[list_of_words[word_index-1],list_of_words[word_index+1]]\n",
    "#    \n",
    "#tokens=[]\n",
    "#for s in text_tokens:\n",
    "#    tokens.extend(word_tokenize(s))\n",
    "#\n",
    "#unique_words=set(tokens)\n",
    "#\n",
    "list_cw_pairs=[]\n",
    "for i in range(0,len(context_word_pairs)):\n",
    "    list_cw_pairs.append(context_word_pairs[i])\n",
    "#\n",
    "#for word in unique_words:\n",
    "#    print(context_word_pairs.get(word))\n",
    "    \n",
    "\n",
    "\n",
    "def foo(r, d):\n",
    "    for k in d:\n",
    "        r[k].append(d[k])\n",
    "    \n",
    "d = reduce(lambda r, d: foo(r, d) or r, list_cw_pairs, defaultdict(list))    \n",
    "    \n",
    "    \n",
    "final_dict={}\n",
    "for k,v in d.items():\n",
    "    tmp_list_before=[]\n",
    "    tmp_list_after=[]\n",
    "    for x in range(0,len(v)):\n",
    "        tmp_list_before.append(v[x][0])\n",
    "        tmp_list_after.append(v[x][1])\n",
    "    final_dict[k]=[tmp_list_before,tmp_list_after]\n",
    "    \n",
    "def create_vectors(word,context):\n",
    "    #print(word)\n",
    "    #print(context)\n",
    "    vector = np.zeros((len(word_dist.keys()),)) \n",
    "    for x in range(0,len(context[0])):\n",
    "        temp_word_1 = context[0][x]+\" \" + word\n",
    "        if temp_word_1 in pmi_dict.keys():\n",
    "            vector[word_dist[context[0][x]]] = pmi_dict[temp_word_1]\n",
    "    for y in range(0,len(context[1])):\n",
    "        temp_word_2 = word + \" \" + context[1][y]\n",
    "        if temp_word_2 in pmi_dict.keys():\n",
    "            vector[word_dist[context[1][y]]] = pmi_dict[temp_word_2]\n",
    "    return vector\n",
    "\n",
    "word_vectors=[]\n",
    "word_list=[]\n",
    "for w,v in final_dict.items():\n",
    "    if w!=\"\":\n",
    "        word_vectors.append(create_vectors(w,v))\n",
    "        word_list.append(w)\n",
    " \n",
    " \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "pca=PCA(n_components=300)\n",
    "\n",
    "principalComponents=pca.fit_transform(word_vectors)\n",
    "\n",
    "'''\n",
    "new_pca = PCA(n_components = 2)\n",
    "n=new_pca.fit_transform(principalComponents)\n",
    "plt.scatter(n[:,0],n[:,1])\n",
    "\n",
    "for i,word in enumerate(word_list[0:300]):\n",
    "    plt.annotate(word,xy=(n[i,0],n[i,1]))\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "labels = []\n",
    "tokens = []\n",
    "\n",
    "for word in range(0,len(word_list)):\n",
    "    tokens.append(word_vectors[word])\n",
    "    labels.append(word_list[word])\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()\n",
    "'''\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "num=300\n",
    "t = AnnoyIndex(num)\n",
    "\n",
    "for i in range(0,len(principalComponents)):\n",
    "    t.add_item(i,principalComponents[i])\n",
    "    \n",
    "t.build(10)\n",
    "    \n",
    "print(word_list[0])\n",
    "#print(t.get_nns_by_item(0,5))\n",
    "for i in t.get_nns_by_item(0,5):\n",
    "    print(word_list[i])\n",
    "#print(t.get_distance(1,4))\n",
    "\n",
    "#print(t.get_distance(18,24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
