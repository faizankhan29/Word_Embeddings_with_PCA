{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of the Repository: (Work in Progress)\n",
    "\n",
    "The idea of the blog/repository is to:\n",
    "\n",
    "1) Explore PCA for text visualizations -- words, sentences, documents.\n",
    "\n",
    "2) Understand how using PCA helps improve text classification.\n",
    "\n",
    "3) Explore the Spotify Annoy library and test the text classification improvement using the same.\n",
    "\n",
    "4) (Optional) Explore the TSNE library for visualization and insights -- words, sentences, documents, text classification outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "train = fetch_20newsgroups(subset=\"train\") \n",
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"data\"]\n",
    "X_test = test[\"data\"]\n",
    "y_train = train[\"target\"] \n",
    "y_test = test[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train, columns=['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  target\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test, columns = ['message'])\n",
    "df_test['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['message'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])',\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_message'] = df_train['message'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>from  lerxst  umd edu  where s my thing  subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>from  guykuo  u washington edu  guy kuo  subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>from  twillis  ecn purdue edu  thomas e willis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>from  jgreen   joe green  subject  re  weitek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>from  jcm  cfa harvard edu  jonathan mcdowell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  from  lerxst  umd edu  where s my thing  subje...  \n",
       "1  from  guykuo  u washington edu  guy kuo  subje...  \n",
       "2  from  twillis  ecn purdue edu  thomas e willis...  \n",
       "3  from  jgreen   joe green  subject  re  weitek ...  \n",
       "4  from  jcm  cfa harvard edu  jonathan mcdowell ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['len'] = df_train['cleaned_message'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>from  lerxst  umd edu  where s my thing  subje...</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>from  guykuo  u washington edu  guy kuo  subje...</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>from  twillis  ecn purdue edu  thomas e willis...</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>from  jgreen   joe green  subject  re  weitek ...</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>from  jcm  cfa harvard edu  jonathan mcdowell ...</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "                                     cleaned_message   len  \n",
       "0  from  lerxst  umd edu  where s my thing  subje...   718  \n",
       "1  from  guykuo  u washington edu  guy kuo  subje...   851  \n",
       "2  from  twillis  ecn purdue edu  thomas e willis...  1976  \n",
       "3  from  jgreen   joe green  subject  re  weitek ...   790  \n",
       "4  from  jcm  cfa harvard edu  jonathan mcdowell ...  1098  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# file=open('data/test.ft.txt','r',encoding=\"utf-8\")\n",
    "\n",
    "# g=file.read()\n",
    "\n",
    "# sentences=sent_tokenize(g)\n",
    "# # \n",
    "# wn=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def preprocess_text(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     stop_words=stopwords.words('english')\n",
    "#     stop_words.extend(['__label__1','__label__2'])\n",
    "#     tokens=[token.lower() for token in tokens if token not in stop_words]\n",
    "#     tokens=[re.sub(r'[^A-Za-z]+','',token) for token in tokens]\n",
    "#     tokens=[wn.lemmatize(token) for token in tokens]\n",
    "#     return tokens  \n",
    "\n",
    "\n",
    "# text_tokens=[]\n",
    "# for item in sentences[0:1000]:\n",
    "#     tokens = preprocess_text(item)\n",
    "#     temp = \" \".join(tokens)\n",
    "#     text_tokens.append(temp)    \n",
    "\n",
    "\n",
    "\n",
    "# word_dist = FreqDist()\n",
    "# for s in text_tokens:\n",
    "#     word_dist.update(s.split())\n",
    "\n",
    "# ########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# text=''\n",
    "# for sent in text_tokens:\n",
    "#     text=text+sent\n",
    "\n",
    "# tokens=word_tokenize(text)    \n",
    "# bigrams = ngrams(tokens,2)\n",
    "\n",
    "# bigram_dict=dict(Counter(bigrams))\n",
    "\n",
    "# final_bigram_dict={}\n",
    "# for key,value in bigram_dict.items():\n",
    "#     new_key=\" \".join(key)\n",
    "#     final_bigram_dict[new_key]=value\n",
    "\n",
    "# unigram_index= CountVectorizer(ngram_range=(1,1))\n",
    "# unigram_index.fit_transform(text_tokens)\n",
    "# unigram_dist = unigram_index.vocabulary_\n",
    "\n",
    "\n",
    "# def pmi(word1, word2 ,unigram_freq, bigram_freq):\n",
    "#     #print(word1,word2)\n",
    "#     prob_word1 = unigram_freq[word1]/float(sum(unigram_freq.values()))\n",
    "#     #print(prob_word1)\n",
    "#     prob_word2 = unigram_freq[word2]/float(sum(unigram_freq.values()))\n",
    "#     #print(prob_word2)\n",
    "#     prob_word1_word2 = bigram_freq[\" \".join([word1,word2])]/float(sum(bigram_freq.values()))\n",
    "#     #print(prob_word1_word2)\n",
    "#     ratio = prob_word1_word2/float(prob_word1*prob_word2)\n",
    "#     #print(word1,word2,prob_word1,prob_word2)\n",
    "#     if ratio==0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return math.log(ratio,2)\n",
    "\n",
    "# pmi_dict={}\n",
    "# for key in final_bigram_dict.keys():\n",
    "#     first_word = key.split()[0]\n",
    "#     second_word = key.split()[1]\n",
    "#     if (first_word in word_dist.keys()) and (second_word in word_dist.keys()):\n",
    "#         pmi_dict[key]=pmi(key.split()[0],key.split()[1],word_dist,final_bigram_dict)\n",
    "#     else:\n",
    "#         pmi_dict[key]=0\n",
    "\n",
    "\n",
    "# start = '\\s'\n",
    "# end= '\\e'\n",
    "\n",
    "    \n",
    "# context_word_pairs={}\n",
    "# for story_id in range(0,len(text_tokens)):\n",
    "#     text_tokens[story_id] = start +' '+ text_tokens[story_id] +' '+ end\n",
    "#     list_of_words=text_tokens[story_id].split()\n",
    "#     context_word_pairs[story_id] = {}\n",
    "#     for word_index in range(1,len(list_of_words)-1):\n",
    "#         context_word_pairs[story_id][list_of_words[word_index]]=[list_of_words[word_index-1],list_of_words[word_index+1]]\n",
    "# #    \n",
    "# #tokens=[]\n",
    "# #for s in text_tokens:\n",
    "# #    tokens.extend(word_tokenize(s))\n",
    "# #\n",
    "# #unique_words=set(tokens)\n",
    "# #\n",
    "# list_cw_pairs=[]\n",
    "# for i in range(0,len(context_word_pairs)):\n",
    "#     list_cw_pairs.append(context_word_pairs[i])\n",
    "# #\n",
    "# #for word in unique_words:\n",
    "# #    print(context_word_pairs.get(word))\n",
    "    \n",
    "\n",
    "\n",
    "# def foo(r, d):\n",
    "#     for k in d:\n",
    "#         r[k].append(d[k])\n",
    "    \n",
    "# d = reduce(lambda r, d: foo(r, d) or r, list_cw_pairs, defaultdict(list))    \n",
    "    \n",
    "    \n",
    "# final_dict={}\n",
    "# for k,v in d.items():\n",
    "#     tmp_list_before=[]\n",
    "#     tmp_list_after=[]\n",
    "#     for x in range(0,len(v)):\n",
    "#         tmp_list_before.append(v[x][0])\n",
    "#         tmp_list_after.append(v[x][1])\n",
    "#     final_dict[k]=[tmp_list_before,tmp_list_after]\n",
    "    \n",
    "# def create_vectors(word,context):\n",
    "#     #print(word)\n",
    "#     #print(context)\n",
    "#     vector = np.zeros((len(word_dist.keys()),)) \n",
    "#     for x in range(0,len(context[0])):\n",
    "#         temp_word_1 = context[0][x]+\" \" + word\n",
    "#         if temp_word_1 in pmi_dict.keys():\n",
    "#             vector[word_dist[context[0][x]]] = pmi_dict[temp_word_1]\n",
    "#     for y in range(0,len(context[1])):\n",
    "#         temp_word_2 = word + \" \" + context[1][y]\n",
    "#         if temp_word_2 in pmi_dict.keys():\n",
    "#             vector[word_dist[context[1][y]]] = pmi_dict[temp_word_2]\n",
    "#     return vector\n",
    "\n",
    "# word_vectors=[]\n",
    "# word_list=[]\n",
    "# for w,v in final_dict.items():\n",
    "#     if w!=\"\":\n",
    "#         word_vectors.append(create_vectors(w,v))\n",
    "#         word_list.append(w)\n",
    " \n",
    " \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# pca=PCA(n_components=300)\n",
    "\n",
    "# principalComponents=pca.fit_transform(word_vectors)\n",
    "\n",
    "# '''\n",
    "# new_pca = PCA(n_components = 2)\n",
    "# n=new_pca.fit_transform(principalComponents)\n",
    "# plt.scatter(n[:,0],n[:,1])\n",
    "\n",
    "# for i,word in enumerate(word_list[0:300]):\n",
    "#     plt.annotate(word,xy=(n[i,0],n[i,1]))\n",
    "# plt.show()\n",
    "\n",
    "# '''\n",
    "# labels = []\n",
    "# tokens = []\n",
    "\n",
    "# for word in range(0,len(word_list)):\n",
    "#     tokens.append(word_vectors[word])\n",
    "#     labels.append(word_list[word])\n",
    "\n",
    "# tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "# new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# for value in new_values:\n",
    "#     x.append(value[0])\n",
    "#     y.append(value[1])\n",
    "    \n",
    "# plt.figure(figsize=(16, 16)) \n",
    "# for i in range(len(x)):\n",
    "#     plt.scatter(x[i],y[i])\n",
    "#     plt.annotate(labels[i],\n",
    "#                  xy=(x[i], y[i]),\n",
    "#                  xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right',\n",
    "#                  va='bottom')\n",
    "# plt.show()\n",
    "# '''\n",
    "# from annoy import AnnoyIndex\n",
    "\n",
    "# num=300\n",
    "# t = AnnoyIndex(num)\n",
    "\n",
    "# for i in range(0,len(principalComponents)):\n",
    "#     t.add_item(i,principalComponents[i])\n",
    "    \n",
    "# t.build(10)\n",
    "    \n",
    "# print(word_list[0])\n",
    "# #print(t.get_nns_by_item(0,5))\n",
    "# for i in t.get_nns_by_item(0,5):\n",
    "#     print(word_list[i])\n",
    "# #print(t.get_distance(1,4))\n",
    "\n",
    "# #print(t.get_distance(18,24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
